# A submission script/guide written by Bryan Gillis, with help from
# Catherine Heymans and Eric Tittley.
#
# This file is a template submission script for a program of mine.
# I run a python script which creates multiple copies of this file,
# each modified to run on a different image. All created copies of
# this file are then submitted with qsub.
#
# I therefore need to set this script up knowing that it may well be
# running against many other copies of itself. I can't do processes
# that might clog up the cluser's I/O connections from every instance
# of this script at once, so I set up each instance to check to
# try and make sure it's the only one performing copies of large files
# at a time.

# First, some PBS commands to set up how this particular script will run

# We need one node per job, 4 processors per job, and local storage on
# the node selected.
#PBS -l nodes=1:ppn=4:localdatadisc

# Tell the local environment to use up to 8 parallel threads
# (My code is only partially parallelized; if it were fully-parallelized,
# I would use the same number here as the number of processors per job.)
export OMP_NUM_THREADS=8

# The wall time for the calculation in HH:MM:SS
#PBS -l walltime=48:00:00

# Where the errors go
#PBS -e job_log.error

# Where the messages go
#PBS -o job_log

# Set up some variables we'll be using in this script

# worker253 is a special worker on stacpolly that can act as a shortcut
# from the nodes performing jobs to the main RAID, where long-term data is
# stored. By using it here, we use a different pathway than is used by
# users for command-line operations. Setting it up with a variable name
# is useful in case its name gets changed in the future; then we only have
# to change it here, not at every point in the script it's used.
HELP_WORKER=worker253

# The directory we want to store our logs in. Here, $USER gets the value
# of the global variable USER. I put it this way in the script so that if
# anyone copies the script and forgets to modify it, they won't accidentally
# copy (or worse, change) my data.
LOG_DIR=/disk4/$USER/Program_Files/PSF_Fitting

# The ID of the worker this job has been assigned to. We can print this
# out in the logs so that if something goes wrong, we can figure out
# which node was responsible. (I'm looking at you, worker045!)
#
# Note the backticks used here - those aren't apostrophes. They mean
# "take the output of the command contained within these"
WORKERID=`hostname`

# The name of a file on the main RAID we'll use to monitor whether or
# not a script is currently copying over files.
LOCKFILENAME=/home/brg/.IO_lockfile

# The name of the image I want to process with this particular instance
# the job. I'll have to copy this file over to the node's local data
# drive so it can be accessed there easily.
IMAGE='ja9bw2a5q_sci1_cor.fits'

# The command line arguments I'm going to pass to my program.
CLINE_ARGS='0.99 26 8 4 True True'

# The folders where the data is stored on the main RAID                                      
REMOTE_DATA_DIR=/disk4/$USER/Data/HST_Fields
REMOTE_IMAGE=$REMOTE_DATA_DIR/$IMAGE

# Where the data is copied to on the local worker                                
LOCAL_DATA_DIR=/data/$USER/HST_Fields
LOCAL_IMAGE=$LOCAL_DATA_DIR/$IMAGE

# Okay, out variables are set up, so now we'll get to actual work.
# Remember that this script is going to be run by a worker, so all commands
# have to be written from its perspective

# Create a directory to store the data we need if it doesn't already exist
mkdir -p $LOCAL_DATA_DIR

# Print the name of the worker we're on to the log
echo 'hostname: '`hostname`

# We might end up running the program many times. It's not worth copying over
# the files we need every time, so let's check if it exists here first before
# doing so.
#
# The below command translates to "If $LOCAL_IMAGE doesn't exist, then..."
if [ ! -f $LOCAL_IMAGE ]; then

	# Okay, we're in this branch, so we need to copy the image here. We want
	# to be careful not to clog up the connections though, so we'll only copy
	# it over if the lockfile on the main RAID doesn't exist. And while we copy,
	# we'll create an instance of the lockfile so no other instance tries to
	# copy at the same time.

	# This command staggers the time when different jobs check for the existence     
	# of a lockfile. This way, if they all start at the same time, they won't
	# all see no lockfile, then start simultaneously.                                                            
	RANDOM_TIME=`echo  $[RANDOM%200]| awk '{print $1/10}'`
	sleep $RANDOM_TIME
	
	# Check for the lockfile periodically. When we no longer see it, immediately
	# create it and proceed.
	locked=1
	while [ $locked = 1 ]
	do
	    sleep 1
	    locked=0
	    if [ -f $LOCKFILENAME ]; then
	        locked=1
	    fi
	done
	touch $LOCKFILENAME
	
	# Print to the lockfile the image we're copying over and the worker we're
	# copying to, just in case something goes wrong, so we can use this info
	# later.
	echo $IMAGE $WORKERID > $LOCKFILENAME
	
	# Copy over the image, using $HELP_WORKER here as a shortcut to the RAID.
	# Using rsync here is generally a lot faster than scp.
	rsync -aW --no-compress $HELP_WORKER:$REMOTE_IMAGE $LOCAL_IMAGE
	
	# Once copying is completed delete the lockfile so another                      
	# process can begin
	rm -f $LOCKFILENAME
	
	# Report to the log that we completed this step successfully
	echo "Copied over field image"

fi

# Now, we're going to do a similar process for the work directory that this
# program will use. However, this comes with one complication: While the images
# aren't likely to change, the work directory may well change. So, I want to 
# keep track of whether it's been changed or not. There are a few ways to do
# this, such as timestamps, but I chose to go with a version file. Whenever
# I make a change to the work directory, I simply increase the number in the
# version file. This is stored on the worker node, and marks what the last
# version copied over was. If it's out of date, the new version will be copied.
# Aside from that, this is all pretty much the same as above.

# The work directory on the main RAID                                
REMOTE_WORK_DIR=/disk4/brg/Program_Files/PSF_Fitting/workdir

# Where the data is copied to on the local worker                                
LOCAL_WORK_DIR=/data/$USER/workdir
LOCAL_WORK_DIR_VERSION_FILE=$LOCAL_WORK_DIR/.v1.7

# Check for the presence of the right version file
if [ ! -f $LOCAL_WORK_DIR_VERSION_FILE ]; then

	# Stagger our check time again.                                                                 
	RANDOM_TIME=`echo  $[RANDOM%200]| awk '{print $1/10}'`
	sleep $RANDOM_TIME
	
	# Start checking for the lockfile before transferring the data
	# Note that we use the same lockfile as before.
	locked=1
	while [ $locked = 1 ]
	do
	    sleep 1
	    locked=0
	    if [ -f $LOCKFILENAME ]; then
	        locked=1
	    fi
	done
	touch $LOCKFILENAME
	
	echo $IMAGE $WORKERID > $LOCKFILENAME
	
	rsync -aW --no-compress $HELP_WORKER:$REMOTE_WORK_DIR/ $LOCAL_WORK_DIR
	
	# Remove any past version files to avoid clutter, then add the updated one
	rm $LOCAL_WORK_DIR/.v*
	echo " " > $LOCAL_WORK_DIR_VERSION_FILE
	
	# Once copying is completed delete the lockfile so another                      
	# process can begin
	
	rm -f $LOCKFILENAME
	
	# Report success of this step
	echo "Copied over work directory"

fi

# Okay, now we should have everything we need to run the program. Let's do that
# now.

# Now set up and run the script
EXE=$LOCAL_WORK_DIR/main.py
IC=$LOCAL_IMAGE' '$CLINE_ARGS

cd $LOCAL_WORK_DIR

echo "Executing command '"$EXE $IC \> $IMAGE.log"' from directory "$LOCAL_WORK_DIR

$EXE $IC > $IMAGE.log

echo 'Finished execution.'

# Finally, we'll want to copy any results back to the main RAID. In this step,
# we don't bother checking if they already exist. We assume that if we're
# running the program, we want the new results.

# Wait until the lockfile doesn't exist, then create it and go ahead
locked=1
while [ $locked = 1 ]
do
    sleep 1
    locked=0
    if [ -f $LOCKFILENAME ]; then
        locked=1
    fi
done
touch $LOCKFILENAME

echo $IMAGE $WORKERID > $LOCKFILENAME

# Copy over the result files. The particular form here is of course unique to
# my program.
rsync -aW --no-compress $LOCAL_DATA_DIR/*stack* $HELP_WORKER:$REMOTE_DATA_DIR
rsync -aW --no-compress $LOCAL_DATA_DIR/*fitting_record* $HELP_WORKER:$REMOTE_DATA_DIR
rsync -aW --no-compress $IMAGE.log $HELP_WORKER:$LOG_DIR

# Once copying is completed delete the lockfile so another                      
# process can begin

rm -f $LOCKFILENAME

echo "Finished copying back output."

# And we're done!

# ...But wait! We've left all our data on the workers we've used. Shouldn't
# we clean up after ourselves when we're done? Yes, we should. So, once
# we're done with the project, we can clean up all the data we've created
# with a script like the below (paste into a new file, uncomment, and run).

# #!/bin/bash
#
# # Erase all data on workers 1 through 54. If at a later date workers
# # beyond 54 get data disks, this will have to be modified.
# # (Cheers to Ami Choi for this script!) 
#
# for w in $(seq 1 9); do
#     ssh worker00${w} rm -rf /data/$USER/*
# done
# 
# for w in $(seq 10 54); do
#     ssh worker0${w} rm -rf /data/$USER/*
# done
